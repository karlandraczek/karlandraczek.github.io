[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Scanning negative films\n\n\n\n\n\n\nphotography\n\n\ngear\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 12: Simple reproductibility in R\n\n\n\n\n\n\nreproducibility\n\n\nR\n\n\ntechnews\n\n\nexperience\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 11: Facilitating… admin tasks\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 10: Pipes\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 9: Testing data\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSaving the camera settings of a shot in the exif data of the scans\n\n\n\n\n\n\nphotography\n\n\nweekend-project\n\n\nexperience\n\n\nR\n\n\npackage\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 8: Sharing code through your own R package\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 7: tidymodels\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow I moved the website to albansagouis.com\n\n\n\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the website online or locally?\n\n\n\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing a new 35mm analog camera\n\n\n\n\n\n\nphotography\n\n\ngear\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMy discovery of analog photography\n\n\n\n\n\n\nphotography\n\n\ngear\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow I created this website\n\n\n\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 6: Creating animated figures\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\nvisualisation\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 5: Working with data files too large for memory\n\n\n\n\n\n\nexperience\n\n\nR\n\n\ndata.table\n\n\nspatial\n\n\ndplyr\n\n\npackage\n\n\nreproducibility\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 4: Reporting your figures and model results in your manuscript\n\n\n\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\nreproducibility\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 3: Building a free website easy\n\n\n\n\n\n\nexperience\n\n\nwebdev\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 2: Publishing code\n\n\n\n\n\n\nreproducibility\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\nexperience\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 1: Spatial packages\n\n\n\n\n\n\nGIS\n\n\nR\n\n\nreproducibility\n\n\nrenv\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "My CV",
    "section": "",
    "text": "Here is my static CV.\n\n\nAnd this is my dynamic CV!\nMy blog posts tagged with experience are automatically collected here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 12: Simple reproductibility in R\n\n\n\nreproducibility\n\n\nR\n\n\ntechnews\n\n\nexperience\n\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 11: Facilitating… admin tasks\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 10: Pipes\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 9: Testing data\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving the camera settings of a shot in the exif data of the scans\n\n\n\nphotography\n\n\nweekend-project\n\n\nexperience\n\n\nR\n\n\npackage\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 8: Sharing code through your own R package\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 7: tidymodels\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow I moved the website to albansagouis.com\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the website online or locally?\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow I created this website\n\n\n\nexperience\n\n\nwebdev\n\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 6: Creating animated figures\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\nvisualisation\n\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 5: Working with data files too large for memory\n\n\n\nexperience\n\n\nR\n\n\ndata.table\n\n\nspatial\n\n\ndplyr\n\n\npackage\n\n\nreproducibility\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 4: Reporting your figures and model results in your manuscript\n\n\n\nexperience\n\n\nR\n\n\npackage\n\n\nreproducibility\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 3: Building a free website easy\n\n\n\nexperience\n\n\nwebdev\n\n\ntechnews\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTech news 2: Publishing code\n\n\n\nreproducibility\n\n\nR\n\n\npackage\n\n\ntechnews\n\n\nexperience\n\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tech-news/2024-07-19-tech-news-12/index.html",
    "href": "posts/tech-news/2024-07-19-tech-news-12/index.html",
    "title": "Tech news 12: Simple reproductibility in R",
    "section": "",
    "text": "Tech News #12? It’s been a year and we are arriving at the end of a cycle as tech news #13 is not planned. It’s been a pleasure writing these 12 newsletters and thinking about what would be the most helpful to others.\n\n\nScientific reproducibility is key to credible and reliable science. It is crucial that we or other scientists can run our own analyses in the future. Sharing code and data is obvious and I will talk about two aspects of our workflow that we can easily change to improve reproducibility: paths and environment.\n\n\n\nTo tell R where a file is, there are two types of paths. One type that works only on 1 computer and one type that works on any computer. The first, not so reproducible paths, are called absolute paths and they begin from the root of your computer such as C:/users/as80fywe/chapter_1/data/my_data.csv (Windows) or ~/chapter_1/data/my_data.csv (Mac, Linux). If you change computer or someone else tries to read these data, this path is going to break and they will have to replace the part of each path that corresponds to your system. On the contrary, a relative path, like ./data/my_data.csv works on any system because it makes reference to the top-level of the project, not to the system root and there are several ways so that R knows where this top-level is.\n\n\nIf you have to use absolute paths, set the project folder only once and read or write data or figures relatively to the project top-level:\n\nsetwd(\"C:/user/as80fywe/chapter_1/project_root\")\nread.csv(\"data/raw_data/my_data.csv\")\nggsave(\"figures/manuscript/fig1.png\")\n\nIs better practice than:\n\nsetwd(\"C:/user/as80fywe/chapter_1/project_root/data/raw_data/\")\nread.csv(\"my_data.csv\")\nsetwd(\"C:/user/as80fywe/chapter_1/project_root/figures/manuscript\")\nggsave(\"fig1.png\")\nsetwd(\"C:/user/as80fywe/chapter_1/project_root\")\n\nTo avoid having to set the working directory in R all together I see two complementary methods. I consider them more or less equally reproducible and both are way more reproducible than ever using setwd().\n\n\n\nR projects are files with the .rproj extension that live in the root folder of each of your projects. They contain a little bit of information about the preferred settings in this project: do you want environment to be saved when closing the session (not great practice), how many spaces in each tab or using magrittr pipe %&gt;% by default instead of base pipe |&gt;. Meaning that in a collaborative workflow, this file automatically homogenise style among the systems of all collaborators. Whenever you want to work on this project, you open this .rproj file which fires up rstudio with a fresh environment and the working directory already set at the root of the project.\nNo need for setwd()\nIt works on any computer\nGood separation between your projects\nNow, if you are not using Rstudio or if you are using rmarkdown or testthat test suites, the R project solution is either not an option or not enough. Also, the exact format of a relative path changes between platforms, i.e. ./data/dat.csv, data/dat.csv or /data/dat.csv may work in some cases but not in others.\n\n\n\nThe here package steps in to shine somewhere where R is of little help: guessing what is the top-level of your project. Since it is pretty good at guessing, you can build a path like this:\nread.csv(here(\"data\", \"raw_data\", \"my_data.csv\"))\nggsave(here(\"figures\", \"manuscript\", \"fig1.png\"))\nHow does here guess what is the top-level folder of your project? It looks for a .here file, then for a .rproj file and if it finds neither, it then look for other cues that this would be a top-level folder. Meaning that if you do not use RStudio and R projects, you could still use here commands and a .here file to build reproducible projects.\nSo to close the Paths section, I would conclude that using R projects helps building reproducible and reliable paths over time and space and facilitates collaborative work.\n\n\n\n\nCrucial aspects of our workflows depend on packages.\n\nSometimes package updates break our code or worse, they give wrong results without warning.\nSometimes we need two versions of the same package for different projects but in R, updating for one always update for all projects.\nSometimes we collaborate on some code and it’s not clear which packages (and which versions) our collaborator is using.\n\nIn all of these situations, R natural behaviour is to install all packages in one place, storing only the latest version and not keeping track of which version is used with each project.\nA solution to all of these problems, simplify our workflow, facilitate collaboration and improve reproducibility could be using the renv package to manage our packages, save the versions and isolate our projects from one another. These different situations demand that we use renv from the beginning of each project and regularly run renv::snapshot().\nLet’s imagine a simpler situation in which a researcher needs to publish reproducible code and make sure that their project will still work in a few weeks when the reviews come back or in a few months when a student tries to run it again. All this researcher needs to do is running renv::init() before publishing the code. When running renv::init(), renv detects the top-level of the project, reads all the code inside the project to list used packages, looks up the versions installed and store the complete list in a renv.lock file. With this list of packages published, another user can install all needed packages at once, and the correct versions, by running renv::restore().\nAnd in the special case where there is a project with code that does not run any more and we want to 1) install the versions from when the code was working, 2) freeze it in time and 3) publish it, renv saves the game again. Find the date when the code was successfully ran last and renv::checkout(date = \"2020-01-02\"). You’ll get the list of correct packages and versions and renv will install them all for you if you run renv::restore()!\nrenv is great, I could not close this cycle without writing about it!\nCheers for renv!\n\n\n\nJenny Brian rant about setwd() and rm(list = ls()): https://www.tidyverse.org/articles/2017/12/workflow-vs-script/\nThe here page: https://here.r-lib.org/\nThe renv package: https://rstudio.github.io/renv/"
  },
  {
    "objectID": "posts/tech-news/2024-07-19-tech-news-12/index.html#what-for",
    "href": "posts/tech-news/2024-07-19-tech-news-12/index.html#what-for",
    "title": "Tech news 12: Simple reproductibility in R",
    "section": "",
    "text": "Scientific reproducibility is key to credible and reliable science. It is crucial that we or other scientists can run our own analyses in the future. Sharing code and data is obvious and I will talk about two aspects of our workflow that we can easily change to improve reproducibility: paths and environment."
  },
  {
    "objectID": "posts/tech-news/2024-07-19-tech-news-12/index.html#paths",
    "href": "posts/tech-news/2024-07-19-tech-news-12/index.html#paths",
    "title": "Tech news 12: Simple reproductibility in R",
    "section": "",
    "text": "To tell R where a file is, there are two types of paths. One type that works only on 1 computer and one type that works on any computer. The first, not so reproducible paths, are called absolute paths and they begin from the root of your computer such as C:/users/as80fywe/chapter_1/data/my_data.csv (Windows) or ~/chapter_1/data/my_data.csv (Mac, Linux). If you change computer or someone else tries to read these data, this path is going to break and they will have to replace the part of each path that corresponds to your system. On the contrary, a relative path, like ./data/my_data.csv works on any system because it makes reference to the top-level of the project, not to the system root and there are several ways so that R knows where this top-level is.\n\n\nIf you have to use absolute paths, set the project folder only once and read or write data or figures relatively to the project top-level:\n\nsetwd(\"C:/user/as80fywe/chapter_1/project_root\")\nread.csv(\"data/raw_data/my_data.csv\")\nggsave(\"figures/manuscript/fig1.png\")\n\nIs better practice than:\n\nsetwd(\"C:/user/as80fywe/chapter_1/project_root/data/raw_data/\")\nread.csv(\"my_data.csv\")\nsetwd(\"C:/user/as80fywe/chapter_1/project_root/figures/manuscript\")\nggsave(\"fig1.png\")\nsetwd(\"C:/user/as80fywe/chapter_1/project_root\")\n\nTo avoid having to set the working directory in R all together I see two complementary methods. I consider them more or less equally reproducible and both are way more reproducible than ever using setwd().\n\n\n\nR projects are files with the .rproj extension that live in the root folder of each of your projects. They contain a little bit of information about the preferred settings in this project: do you want environment to be saved when closing the session (not great practice), how many spaces in each tab or using magrittr pipe %&gt;% by default instead of base pipe |&gt;. Meaning that in a collaborative workflow, this file automatically homogenise style among the systems of all collaborators. Whenever you want to work on this project, you open this .rproj file which fires up rstudio with a fresh environment and the working directory already set at the root of the project.\nNo need for setwd()\nIt works on any computer\nGood separation between your projects\nNow, if you are not using Rstudio or if you are using rmarkdown or testthat test suites, the R project solution is either not an option or not enough. Also, the exact format of a relative path changes between platforms, i.e. ./data/dat.csv, data/dat.csv or /data/dat.csv may work in some cases but not in others.\n\n\n\nThe here package steps in to shine somewhere where R is of little help: guessing what is the top-level of your project. Since it is pretty good at guessing, you can build a path like this:\nread.csv(here(\"data\", \"raw_data\", \"my_data.csv\"))\nggsave(here(\"figures\", \"manuscript\", \"fig1.png\"))\nHow does here guess what is the top-level folder of your project? It looks for a .here file, then for a .rproj file and if it finds neither, it then look for other cues that this would be a top-level folder. Meaning that if you do not use RStudio and R projects, you could still use here commands and a .here file to build reproducible projects.\nSo to close the Paths section, I would conclude that using R projects helps building reproducible and reliable paths over time and space and facilitates collaborative work."
  },
  {
    "objectID": "posts/tech-news/2024-07-19-tech-news-12/index.html#renv-saving-the-environment",
    "href": "posts/tech-news/2024-07-19-tech-news-12/index.html#renv-saving-the-environment",
    "title": "Tech news 12: Simple reproductibility in R",
    "section": "",
    "text": "Crucial aspects of our workflows depend on packages.\n\nSometimes package updates break our code or worse, they give wrong results without warning.\nSometimes we need two versions of the same package for different projects but in R, updating for one always update for all projects.\nSometimes we collaborate on some code and it’s not clear which packages (and which versions) our collaborator is using.\n\nIn all of these situations, R natural behaviour is to install all packages in one place, storing only the latest version and not keeping track of which version is used with each project.\nA solution to all of these problems, simplify our workflow, facilitate collaboration and improve reproducibility could be using the renv package to manage our packages, save the versions and isolate our projects from one another. These different situations demand that we use renv from the beginning of each project and regularly run renv::snapshot().\nLet’s imagine a simpler situation in which a researcher needs to publish reproducible code and make sure that their project will still work in a few weeks when the reviews come back or in a few months when a student tries to run it again. All this researcher needs to do is running renv::init() before publishing the code. When running renv::init(), renv detects the top-level of the project, reads all the code inside the project to list used packages, looks up the versions installed and store the complete list in a renv.lock file. With this list of packages published, another user can install all needed packages at once, and the correct versions, by running renv::restore().\nAnd in the special case where there is a project with code that does not run any more and we want to 1) install the versions from when the code was working, 2) freeze it in time and 3) publish it, renv saves the game again. Find the date when the code was successfully ran last and renv::checkout(date = \"2020-01-02\"). You’ll get the list of correct packages and versions and renv will install them all for you if you run renv::restore()!\nrenv is great, I could not close this cycle without writing about it!\nCheers for renv!"
  },
  {
    "objectID": "posts/tech-news/2024-07-19-tech-news-12/index.html#resources",
    "href": "posts/tech-news/2024-07-19-tech-news-12/index.html#resources",
    "title": "Tech news 12: Simple reproductibility in R",
    "section": "",
    "text": "Jenny Brian rant about setwd() and rm(list = ls()): https://www.tidyverse.org/articles/2017/12/workflow-vs-script/\nThe here page: https://here.r-lib.org/\nThe renv package: https://rstudio.github.io/renv/"
  },
  {
    "objectID": "posts/tech-news/2024-04-26-tech-news-10/index.html",
    "href": "posts/tech-news/2024-04-26-tech-news-10/index.html",
    "title": "Tech news 10: Pipes",
    "section": "",
    "text": "What for?\nPipes are present is many languages and they allow passing objects from one function to another without having to create an intermediary object and keeping a logical and readable flow. In the command line, you can read all files names in a folder, pass them to grep to select all “.R” files and pass them all to Rscript to execute them. Instead of creating intermediate files, the result of each function is passed to the next by a pipe, represented by “|” in bash.\nIn R, there were two classical ways of going through a workflow. Creating intermediate objects even though not useful to keep:\n# Abundances in three sites\ndt &lt;- data.frame(abundance = 20:50,\n                 site = rep(\n                    c(\"A\", \"B\", \"C\"),\n                    each = 10))\n\nsitesAB &lt;- subset(dt, site %in% c(\"A\", \"B\"))\nsubSample &lt;- sample(sitesAB$abundance, size = 5)\nsortedAbundance &lt;- sort(subSample,\n                        descending = FALSE)\nOr nesting functions with the disadvantage that you have to read the workflow from the inside to the outside and arguments of the same function are sometimes far from each other, a hard-to-digest-sandwich…\nsortedAbundance &lt;- sort(\n                     sample(\n                       subset(dt,\n                              site %in% c(\"A\", \"B\"))),\n                       size = 5),\n                     descending = FALSE)\nUsing a pipe, it looks like this:\nsortedAbundance &lt;- dt %&gt;%\n  subset(site %in% c(\"A\", \"B\")) %&gt;%\n  sample(size = 5) %&gt;%\n  sort(descending = FALSE)\nEasier to read, intention is clear, arguments stay closer to each other and input and output objects are also close to each other: dt is transformed into sortedAbundance.\nThe pipe takes the object passed to it and passes it as the first argument of the next function. If we need to pass the piped object to the second argument, we can name arguments or use a place holder:\nc(\"A\",\"B\") %&gt;% sub(pattern = “A”,\n                   replacement = “”)\nc(\"A\",\"B\") %&gt;% sub(pattern = “A”,\n                   replacement = “”,\n                   x = .)\n\n\nThe magrittr pipe\nThe magrittr package with its iconic %&gt;% pipe, was first published early 2014, apparently it caught traction pretty quick and Rstudio developers contacted the creator: “We also worked on a pipe %.% but it’s not as functional, practical and rich as yours, could we collaborate?”\nThis pipe, together with the tidyverse grammar then revolutionised the R ecosystem…\n\n\nThe base pipe\nAnd eventually, R developed its own native pipe |&gt;. In appearance, its usage is very similar with one difference being that the placeholder is “_” instead of “.”. You can read more in this Hadley Wickam article or in the pipe section of his book in which he recommends base pipe over magrittr pipe:\n\nFor simple cases, |&gt; and %&gt;% behave identically. So why do we recommend the base pipe? Firstly, because it’s part of base R, it’s always available for you to use, even when you’re not using the tidyverse. Secondly, |&gt; is quite a bit simpler than %&gt;%: in the time between the invention of %&gt;% in 2014 and the inclusion of |&gt; in R 4.1.0 in 2021, we gained a better understanding of the pipe. This allowed the base implementation to jettison infrequently used and less important features.\n\nPartly because the base pipe is simpler, it has no overhead and is much faster than the magrittr pipe:\nR&gt; system.time({for(i in 1:1e5) identity(x)})\n   user  system elapsed \n  0.015   0.000   0.015 \nR&gt; system.time({for(i in 1:1e5) x |&gt; identity()})\n   user  system elapsed \n  0.015   0.000   0.015 \nR&gt; system.time({for(i in 1:1e5) x %&gt;% identity()})\n   user  system elapsed \n  0.105   0.001   0.106\n\n\nThe other pipes\nLess frequent as the well-known forward pipe %&gt;%, the magrittr package offers other pipes!\n\nThe assignment pipe %&lt;&gt;%: Pipe an object forward into a function or call expression and update the left-hand-side object with the resulting value.\n\ndt %&lt;&gt;% mean() # equivalent to dt &lt;- dt %&gt;% mean()\n\nThe exposition pipe %$%: Expose the names in left-hand-side to the right-hand-side expression. This is useful when functions do not have a built-in data argument.\n\niris %&gt;%\n  subset(Sepal.Length &gt; mean(Sepal.Length)) %$%\n  cor(Sepal.Length, Sepal.Width)\n#&gt; [1] 0.3361992\n\nThe tee pipe %T%: Pipe a value forward into a function- or call expression and return the original value instead of the result. This is useful when an expression is used for its side-effect, say plotting or printing.\n\nrnorm(200) %&gt;%\n  matrix(ncol = 2) %T&gt;%\n  plot() %&gt;% \n  colSums()\n\n\nResources\nThe pipe article by Hadley Wickam: https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/\nThe pipe article in the R for Data Science book (Hadley Wickam, second edition): https://r4ds.hadley.nz/data-transform.html#sec-the-pipe"
  },
  {
    "objectID": "posts/tech-news/2024-02-16-tech-news-8/index.html",
    "href": "posts/tech-news/2024-02-16-tech-news-8/index.html",
    "title": "Tech news 8: Sharing code through your own R package",
    "section": "",
    "text": "This month, I thought of writing a little bit about sharing code with each other.\n\nCollaborating on code\nLet’s imagine having a workshop with a group of colleagues and over the course of the week, some will develop tools and some will use said tools for various analyses. The workflow will most likely be:\n\nlet’s define our needs, some functions, the data they take in input and what we all need out\nsome begin writing functions, each their own maybe and then share with everyone,\nthe code is emailed around, saved on a USB key or shared on DropBox\n\nusers test the functions, find bugs, need improvements, need more details on input format, give feedback\n\n\nIn the meantime, programers fixed some things, fixed functions, created others and back to 2.\n\n\nIt’s hard keeping track of the versions and consistently and easily distributing updates to users. It’s hard distributing documentation, testing functions reproducibly and controlling the dependencies. And if you’re teaching and want to distribute code and data all in one?\n\n\nMake your life easier, make a package\nIf the functions were written in a package, documentation and tests can easily be written and processed thanks to rstudio built-in tools. The users can install the package, from the shared dropbox or from github/gitlab almost as usual, and load it with library(yourPackage) as usual. They can access the help documentation you wrote with “?” as they are used to, read examples, enjoy auto-completion of the arguments, etc. When they give you feedback, the version is clear and the code changes can be tested in seconds with a shortcut.\nYou can also share data inside the package, and again, the version will be a timestamp and get you a reproducibility badge. It’s also going to be very easy to access for the users:\nlibrary(yourPackage)\ndataName\n\n\nHow to make a package?\nThere are so many great (better) resources online! But let me just give a couple of tips:\nThe absolute minimum that makes an R package an R package is one DESCRIPTION file and one R script, that’s it!\nBegin small, discover the thrill, clarity and comfort of writing documentation and tests for your functions. This will give you ways of imagining edge cases, eg thinking of the missing values, and controlling. Make a change, Ctrl+Shift+T(est), make another change, Ctrl+Shift+T, etc!\n\nMake a change, Ctrl+Shift+T(est), make another change, Ctrl+Shift+T, etc!\n\nTake advantage of tools you already have on your computer: RStudio, the devtools and testthat packages for example.\nFor whatever questions you might have while building, documenting or testing, the first DuckDuckGo result will very likely comes from Wickham’s book (https://r-pkgs.org/), it’s just so complete…\nWanna do it differently? Build the tests first to build up the framework of exactly how you want your function to behave, then write the function.\n\n\nSome useful tools\n\ndevtools::document() will build the documentation, help pages, vignettes everything\ndevtools::install(), devtools::install(“/mydropbox/myPackage”) or devtools::install_github(“yourRepo/yourPackage”) will install the package from source from wherever is the code.\ntestthat::test_package() will run all of your tests and give you encouraging words when they fail, how thoughtful!\n\nI hope this decreased the barrier of trying out building a package. I hope however that it is not decreasing the impressiveness of it!\nBest wishes,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2023-12-19-tech-news-6/index.html",
    "href": "posts/tech-news/2023-12-19-tech-news-6/index.html",
    "title": "Tech news 6: Creating animated figures",
    "section": "",
    "text": "This week, some fun with animated figures. We send each other gifs but they rarely come to mind when it comes to showing scientific processes or results. They are actually pretty easy to make in R!\nHere are a few examples using gganimate. There are other packages out there but gganimate is really well integrated with ggplot2."
  },
  {
    "objectID": "posts/tech-news/2023-12-19-tech-news-6/index.html#footnotes",
    "href": "posts/tech-news/2023-12-19-tech-news-6/index.html#footnotes",
    "title": "Tech news 6: Creating animated figures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPedersen T, Robinson D (2022). gganimate: A Grammar of Animated Graphics. https://gganimate.com, https://github.com/thomasp85/gganimate.↩︎\nPedersen T, Robinson D (2022). gganimate: A Grammar of Animated Graphics. https://gganimate.com, https://github.com/thomasp85/gganimate.↩︎\nDavid J. Lilja, “How to Plot and Animate Data on Maps Using the R Programming Language,” University of Minnesota Digital Conservancy, https://hdl.handle.net/11299/220339, June, 2021.↩︎"
  },
  {
    "objectID": "posts/tech-news/2023-10-20-tech-news-4/index.html",
    "href": "posts/tech-news/2023-10-20-tech-news-4/index.html",
    "title": "Tech news 4: Reporting your figures and model results in your manuscript",
    "section": "",
    "text": "This month the audience is increasing, besides the Biodiversity Synthesis group, the iCode Coding Club mailing list will receive this email (and the next).\nToday’s email will include tips and ideas on streamlining figure inclusion and reporting model results in manuscripts. Since practices in the team and the institute vary a lot, I’ll give information for different levels, mostly for colleagues who use copy-pasting / hand-typing but latex/rmarkdown/quarto users might also learn something useful (or teach me something in return :D).\n\nReporting figures\nHere, I essentially recommend not to ever copy-paste:\n\nAlways save your figures using the appropriate function, ggsave, pdf, png, jpg, etc. This is the better move for a streamlined and reproducible workflow. I like using a vectorised format such as SVG, finally supported by Apple Pages and Keynote, as it is lightweight and have no resolution problem but journals often don’t want them?\nThen from your Word or Powerpoint document, Insert, Pictures, Picture from file, select your file and select the options “Link to File” and “Save with Document” and Insert. Thanks to these options, every time your R script updates the figure file on your disk, Word or Powerpoint will update the figure in the document (maybe saving and reopening needed to see the changes hehehe). I find this feature great but can’t replicate it on Pages or Keynote.\n\n\n\nReporting tables\n\nMethod 1\nFirst, a few functions that will help you format text that you might occasionally need to copy-paste between R and your favourite text editor.\n\nbase::format() or base::prettyNum() to format numeric values with the exact number of decimals or scientific number formatting you want or big number separator as in 1,234,567.90\n\nYou can ask R to show up to 50 decimals, so here is pi!\nbase::formatC(pi, 50)\n&gt; 3.141592653589793115997963468544185161590576171875\n\nbase::format() or stringr::str_pad() (tidyverse) to align text or add “0” strings before numbers to get 01, 02, …, 09, 10, 11\nbroom::tidy() (tidyverse) will give you model results in a much more manageable format than base::summary(). See attached screenshot. \nThe glue package with its central function glue() offers a smarter tidyverse alternative to base::paste()\n\n\n\nMethod 2 AKA the reproducible method\nThe second method I recommend to users who copy-paste / type results in tables in their Powerpoint or Word documents: using R language to directly create the tables. I should keep it simple and present only one but I see a strong advantage in each of these two packages: flextable and gt.\n\nThe flextable package\nIt’s a highly customisable tool to turn a data.frame or matrix or tibble or data.table or other types of table-like objects in a fully formatted table with borders, justified text, merged cells for titles, legend and all. The functions have clear names and using a pipe, it would look like the code for a ggplot. I’ll only give an example and the associated result:\n  library(flextable)\n\n  ft &lt;- flextable(airquality[ sample.int(10),])\n  ft &lt;- add_header_row(ft,\n    colwidths = c(4, 2),\n    values = c(\"Air quality\", \"Time\")\n  )\n  ft &lt;- theme_vanilla(ft)\n  ft &lt;- add_footer_lines(ft, \"Daily air quality measurements in New York, May to September 1973.\")\n  ft &lt;- color(ft, part = \"footer\", color = \"#666666\")\n  ft &lt;- set_caption(ft,\n                    caption = \"New York Air Quality Measurements\")\n  ft\nAnd a screenshot of the corresponding table: \n\n\nThe gt package\nThe gt package is a Rstudio package and integrates naturally with tidyverse. It also profits from a large community of users and a small universe of extensions such as gtsummary and gtExtras. The gt creators provide a video describing some of the features and functions: \nHere is an example they provide with code integrated with dplyr functions:\n  library(gt)\n  # Define the start and end dates for the data range\n  start_date &lt;- \"2010-06-07\"\n  end_date &lt;- \"2010-06-14\"\n\n  # Create a gt table based on preprocessed\n  # `sp500` table data\n  sp500 |&gt;\n    dplyr::filter(date &gt;= start_date & date &lt;= end_date) |&gt;\n    dplyr::select(-adj_close) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = \"S&P 500\",\n      subtitle = glue::glue(\"{start_date} to {end_date}\")\n    ) |&gt;\n    fmt_currency() |&gt;\n    fmt_date(columns = date, date_style = \"wd_m_day_year\") |&gt;\n    fmt_number(columns = volume, suffixing = TRUE)\nThis code directly produces this table: \n\n\n\nNow the workflow part!\nIn both cases you’ll begin in your script, maybe have a dedicated script that produces figures and another one for tables, using gt or flextable to create formatted tables. Then I see different ways:\n\nFor Powerpoint. You save your tables as images. flextable only, points for flextable! You then directly include the table in your presentation using the ‘Insert with link’ option I described in the section Reporting figures.\n\n\nsave_as_image(ft, path = \"/path/to/file.png\")\nZero (0) manipulations to do once the script is written…\n\n\nFor Word.\n\nYou’re fine having all of your tables seating in a separate docx document. flextable for the win again:\n\nsave_as_docx(\"my table 1\" = ft1, \"my table 2\" = ft2, path = \"/path/to/file.docx\")\nZero (0) manipulations to do once the script is written…\n\nYou want the table in your manuscript, exactly where it is useful (if Words let you ;) ). Then it seems that you’ll have to copy paste into your own document from the viewer in Rstudio or from the produced Word, Powerpoint or pdf document you produced.\n\nFor both Powerpoint and Word, you can use functions from the officer package to add slides or pages to your documents and include your tables there. Crazy but I see this as an option only when nothing will change in your results anymore and it seems to me that there is always a last last change to make… But a direction to explore still, with flextable.\n\n\n\n\nConclusion\nIn any case, all these workflows save you from typing (highly error-prone) any result. The next step I see would be a single markdown scripts with code for all your figures and tables reliably and reproducibly saving a single docx or pdf document.\nIn the end, flextable also has a little universe of complementary packages and great functions and won the game…\nI hope you learned something new or thought about making your workflow more streamlined and reproducible. Happy to hear about what you think about the topic, the packages presented or anything I missed, and to collaborate with you trying to set these in your workflow.\nBest wishes,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2023-08-18-tech-news-2/index.html",
    "href": "posts/tech-news/2023-08-18-tech-news-2/index.html",
    "title": "Tech news 2: Publishing code",
    "section": "",
    "text": "Disclaimer: this blog post is my opinion, at the time of writing, and might not reflect the opinion of my employer, or mine today.\n\nI would like to talk a little bit about sharing and archiving code. And data, and figures. People want to see it when you publish and your future self does not want to look for the final version in a cascade of folders. And a tip for double-blind submission at the end!\n\nPublishing code\nTo begin, let’s focus on the primary platform: GitHub. While the tendency may be to upload code only upon publication, consider the advantages of maintaining a GitHub repository from the project’s inception. Not only does this foster collaboration, but it also provides a comprehensive history, promoting transparency and enabling retrospective analysis of decisions made.\n\n\nArchiving code\nFor a seamless archiving process, I recommend establishing a connection between GitHub and Zenodo. This integration automatically archives new versions of your repository whenever necessary. Particularly valuable for evolving data sets used across multiple publications, this approach streamlines the documentation process. Comprehensive instructions on setting up this connection can be found here\nTo elaborate briefly, the steps involve:\n\nGranting Zenodo access to your GitHub repositories within your Zenodo account.\nCreating a release on GitHub, e.g., “Version 1 - Metacommunity Surveys - Nature Article by Wubing Xu.”\nZenodo will automatically archive the repository at the time of the release and assign a DOI for proper citation. A noteworthy bonus is the CITATION.cff file, ensuring accurate attribution to all contributors. Further information can be found in Github documentation and here is an example.\n\nShould you opt for a more straightforward approach, consider utilizing Zenodo directly. This method entails uploading content and obtaining a DOI, allowing for future updates and simplified acknowledgment of collaborators.\nWhile I’ve focused on Zenodo, please note that Figshare offers similar functionality and may be an alternative to explore.\n\n\nDouble blind review\nWhen submitting a paper, if the editor asks you anonymised code, you can use Anonymous GitHub. This tool anonymizes your GitHub repository by replacing any identifying information, enabling you to share code anonymously with editors and reviewers during the submission process.\nhttps://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content#repository\n\n\nConclusion\nEfficient code sharing and archiving not only enhance collaboration but also ensure the integrity of our research efforts. By implementing these practices, we collectively contribute to the advancement of our field. Thank you for your attention, and let’s continue to empower each other through shared knowledge.\nBest wishes,\nAlban"
  },
  {
    "objectID": "posts/photography/2024-01-15-choosing-camera/index.html",
    "href": "posts/photography/2024-01-15-choosing-camera/index.html",
    "title": "Choosing a new 35mm analog camera",
    "section": "",
    "text": "The Nikon FA checked quite a lot of my criteria and it’s among the cheapest! \n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "posts/photography/2024-02-25-saving-shot-metadata/index.html",
    "href": "posts/photography/2024-02-25-saving-shot-metadata/index.html",
    "title": "Saving the camera settings of a shot in the exif data of the scans",
    "section": "",
    "text": "For decades (centuries?) photographers have been writing their camera settings for each shot on a notebook to learn and train their eye, to organise their shots or because they are data-freaks. Today, you could still make notes by hand or use a dedicated smartphone app or you could be the proud owner of a more modern film camera that saves the shot data on a memory card.\nOnce the film developed, by yourself or a lab, and scanned, by yourself or a lab, the scans you get do not have the information of the original shots. These data are on paper, on a smartphone or on a memory card.\nI downloaded the Analog app for my smartphone, because it is free, does not collect user data, it’s lightweight and I loved the design, and I began saving the settings for each shot. Data was accumulating but I was not seeing how to get it out yet.\nA digital camera would take the picture and save the settings inside the jpg or tiff files in exif data. I want the same thing for my scans!\nAnd how to get these data in the exif fields of the scans? Well surprisingly, it’s not as straightforward as you (I) would expect. At first I did not find anything satisfying and discussing with the developers behind Analog made it clear that it was not a straightforward task for them either. We talked about workflow.\nThe team were encouraging amateur and professional photographers to rename their files, well their scans, following this convention to save the settings of each shot. Then they developed Analog which of course allows users to save Shutter speed, Aperture, the lens’ focal length and exposure correction. At the end of the roll, the data is extracted and the user receives a list of name files by email:\n\nTest roll_NO01_SS50_A2.8_FL50_EX0\nTest roll_NO02_SS50_A2.8_FL50_EX0\nTest roll_NO03_SS125_A4_FL50_EX0\nTest roll_NO04_SS125_A4_FL50_EX0\n\nThey hand-rename the scans or use a renaming software. OK. but this does not go inside the files, a file name is not an ideal way of storing data even when using the standardised convention they created: NOSSAFLEX\n\nIt’s as easy as the name – NOSSAFLEX has all of the information in the title.\nNO = Number\nSS = Shutter Speed\nA = Aperture\nFL = Focal Length\nEX = Exposure\n\nI could not find a way to automatically save the data on my phone to the exif slots of my scans…\nSo I decided to create an R package to do it! An exciting weekend project and certainly an article for the blog!\nSince I was already using the Analog app, I had to follow its data structure."
  },
  {
    "objectID": "posts/photography/2024-02-25-saving-shot-metadata/index.html#analog-photography-metadata",
    "href": "posts/photography/2024-02-25-saving-shot-metadata/index.html#analog-photography-metadata",
    "title": "Saving the camera settings of a shot in the exif data of the scans",
    "section": "",
    "text": "For decades (centuries?) photographers have been writing their camera settings for each shot on a notebook to learn and train their eye, to organise their shots or because they are data-freaks. Today, you could still make notes by hand or use a dedicated smartphone app or you could be the proud owner of a more modern film camera that saves the shot data on a memory card.\nOnce the film developed, by yourself or a lab, and scanned, by yourself or a lab, the scans you get do not have the information of the original shots. These data are on paper, on a smartphone or on a memory card.\nI downloaded the Analog app for my smartphone, because it is free, does not collect user data, it’s lightweight and I loved the design, and I began saving the settings for each shot. Data was accumulating but I was not seeing how to get it out yet.\nA digital camera would take the picture and save the settings inside the jpg or tiff files in exif data. I want the same thing for my scans!\nAnd how to get these data in the exif fields of the scans? Well surprisingly, it’s not as straightforward as you (I) would expect. At first I did not find anything satisfying and discussing with the developers behind Analog made it clear that it was not a straightforward task for them either. We talked about workflow.\nThe team were encouraging amateur and professional photographers to rename their files, well their scans, following this convention to save the settings of each shot. Then they developed Analog which of course allows users to save Shutter speed, Aperture, the lens’ focal length and exposure correction. At the end of the roll, the data is extracted and the user receives a list of name files by email:\n\nTest roll_NO01_SS50_A2.8_FL50_EX0\nTest roll_NO02_SS50_A2.8_FL50_EX0\nTest roll_NO03_SS125_A4_FL50_EX0\nTest roll_NO04_SS125_A4_FL50_EX0\n\nThey hand-rename the scans or use a renaming software. OK. but this does not go inside the files, a file name is not an ideal way of storing data even when using the standardised convention they created: NOSSAFLEX\n\nIt’s as easy as the name – NOSSAFLEX has all of the information in the title.\nNO = Number\nSS = Shutter Speed\nA = Aperture\nFL = Focal Length\nEX = Exposure\n\nI could not find a way to automatically save the data on my phone to the exif slots of my scans…\nSo I decided to create an R package to do it! An exciting weekend project and certainly an article for the blog!\nSince I was already using the Analog app, I had to follow its data structure."
  },
  {
    "objectID": "posts/photography/2024-02-25-saving-shot-metadata/index.html#here-is-the-readme-of-my-package",
    "href": "posts/photography/2024-02-25-saving-shot-metadata/index.html#here-is-the-readme-of-my-package",
    "title": "Saving the camera settings of a shot in the exif data of the scans",
    "section": "Here is the README of my package",
    "text": "Here is the README of my package\nThis package relies on a file naming convention for, essentially analog, photography. Photographs taken on film, then developed, then scanned are lacking important metadata or if they have, it’s from the scanning device, not the original camera. The nossaflex package takes NOSSAFLEX structured file names and can 1) batch rename corresponding pictures and 2) edit their exif data to add information such as focal length, shutter speed, aperture. This allow a photographer to take notes on the Analog app while shooting pictures, export nossaflex compatible file names and use R to rename scans and edit their exif metadata with corresponding shot information.\nThe nossaflex package is the entry of the NOSSAFLEX file naming convention into the R universe to help scientific, professional or amateur photographers to name their picture files consistently and informatively.\nWhen taking a picture with an analog camera, data such as aperture, shutter speed or focal length are not automatically saved the way they are in a digital camera. Many photographers write down these precious metadata in a notebook, we want to help them improve their workflow and data quality.\n\nWhat is NOSSAFLEX?\nHere is an explanation from the creators:\n\nIt’s as easy as the name – NOSSAFLEX has all of the information in the title.\nNO = Number\nSS = Shutter Speed\nA = Aperture\nFL = Focal Length\nEX = Exposure\n\nNOSSAFLEX file names looks like this: NO03_SS250_A8_FL80_EX0.jpg or this: NO34_SS30_A2.8_FL35_EX+1.tiff!\nLearn more on their website or on their Youtube channel.\n\n\nThe package\nHere are the two main functions in the package:\n\nrenaming_nossaflex batch-renames picture files from uninformative DSC_00345.jpg to information-rich NOSSAFLEX name based on data provided by the user, see {analog} section: NO03_SS250_A8_FL80_EX0.jpg.\nediting_exif batch-saves the metadata of the pictures into the exif slots of the scan files (jpg, tiff, etc).\n\n\n\nAnalog or an other app\n\n\nThe workflow\n\n\nInstallation\nYou can install the development version of nossaflex from GitHub with:\n# install.packages(\"devtools\")\n# devtools::install_github(\"AlbanSagouis/nossaflex\")\n\n\nExample\nThis is a basic example which shows you how to solve a common problem:\nlibrary(nossaflex)\nfiles &lt;- c(\"Pictures/2024/01 02 Winter in Berlin/DSC_001034\",\n           \"Pictures/2024/01 02 Winter in Berlin/DSC_001035\",\n           \"Pictures/2024/01 02 Winter in Berlin/DSC_001036\")\nfilenames &lt;- reading_nossaflex(path = \"path_to_the_filenames.txt\") # provided by the `analog` app\nrenaming_nossaflex(filenames = filenames, files = files)\nAdditionally you may want to safely save the shots metadata inside the scans:\nmetadata &lt;- reading_nossaflex(path = \"path_to_the_filenames.txt\") |&gt;  # provided by the `analog` app\n     parsing_nossaflex()\nediting_exif(files, metadata)\n\n\nRelated work\nThe package relies heavily on the great exiftoolr package by @JoshOBrien which itself depends on the great exiftool software by Phil Harvey.\nFinally, jExifToolGUI also offers exif editing and with a Graphical Interface, nice."
  },
  {
    "objectID": "posts/tech-news/2023-07-21-tech-news-1/index.html",
    "href": "posts/tech-news/2023-07-21-tech-news-1/index.html",
    "title": "Tech news 1: Spatial packages",
    "section": "",
    "text": "Hi team,\nThere is important news I want to share with you and I thought I could make this a regular thing so every third Friday of the month, I’ll send news that I think are useful for us all.\nFor this first ’tech news’ mail, the programmed deprecation of the most widely used spatial packages. You’ve heard about it but it’s getting closer so let’s talk about it.\n\n\nWhich packages will be deprecated?\nIn 2-3 months from now rgeos, rgdal and maptools will be withdrawn from CRAN. The sp and raster packages are affected and will be superseded by sf and terra respectively: &gt; The sp package is not going to be removed from CRAN soon, but it is good to stop using it because it is no longer being actively developed.\nDozens of other packages using them will also be affected increasing the chances that you will be affected. Here are some packages that you might be using that depend on sp in the background: CoordinateCleaner, RgoogleMaps, ggOceanMaps, ecospat, leaflet.\n\n\nWhat can we do?\nThe developers are trying to make things easier for us by changing things in the background: sp will not use rgdal and rgeos any more but sf. Just updating sp in your system and having sf installed could work without you having to make any change to your scripts but most likely things will break and checks are necessary.\nIn new code, the r spatial consortium strongly encourages you to switch to sf or terra and not use sp and raster any more.\nYou can look into your old projects, looking for “sp”, “rgdal”, “rgeos” or “maptools”, using Ctrl-Shit-F (Or Cmd-Shift-F) in Rstudio or your system explorer and replace them with the equivalent sf functions smartly listed here: rgdal-retirement . If you decide not to update the old projects, you could use renv to save rgdal and rgeos packages in your project but even this would eventually break as they depend on other resources in your system that are not saved by renv.\n\nResources:\n\nThe r-spatial book\nThe Applied Spatial Data Analysis with R book has examples using sp and sf or raster and terra to see the differences.\n\nI would be happy to help with this transition or with any other repo structure/function writing/code problem solving you might have.\nCheers,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2023-09-22-tech-news-3/index.html",
    "href": "posts/tech-news/2023-09-22-tech-news-3/index.html",
    "title": "Tech news 3: Building a free website easy",
    "section": "",
    "text": "This month, I thought of giving you some basic info on how to build a website the easy way to have a personal space where to advertise yourself or your current project. Github will help again, they offer the place to store the code, the engine that builds the website and they even host your website, all for free.\n\nThe Creation\nIt’s called Github Pages and they are doing an excellent job at explaining how to create, modify and personalise your website. They have a step by step guide here. And it’s only 5 steps! If could summarise it even more:\n\nGo to GitHub.com, create a public repository called username.github.io\nFrom GitHub.com or from your local machine, crate a file index.html and commit (and push it if you’re on your computer)\nGitHub Pages will build and publish your website at the address https://username.github.io\n\nThen every time you will make a change, add a page, add an image and commit, the website is going to be automatically updated! Pages are written using Markdown, maybe you are already familiar with this way of writing formatted text, it’s basic and intuitive. I go here when looking for help on Markdown. Github Pages will translate this readable and simple markdown into html.\n\n\nAdding content\nAt this stage, the website is completely empty and creating a distinctive style and all pages (About me, My publications, My projects, My data sets, My cooking recipes, Hire me…) could be a challenge but I have a couple of tips to help you get started:\n\nYou can just copy paste somebody’s website and replace only the text that you need… Emma and Dani use Github Pages for their personal websites (see also Matthias’) and Emma is building one for the Restore Project\nUse a theme that will give you more freedom than the few themes found in the Github Pages settings. Emma recommends Minimal Mistakes, you can directly fork their repository in your Github account and get an entire template/skeleton to play with and they have very good documentation on how to do just that.\nCommit often to detect any errors. From that point of view, it helps working from your computer where you could have a more complex setup and build the website locally to avoid having remote servers working so intensively. It would also be more interactive: the website is rebuild each time you save a modification, you don’t even need to commit changes to see your website change.\n\n\n\nJekyll\nA more technical note, the engine that translates your Markdown pages into HTML is called Jekyll. It was developed for blogging but works well for websites focused on pages too. If you choose to build a website and begin making changes to the style or the structure, you’ll encounter documentation about Jekyll, it means you’re at the right place.\nWhile helping Emma moving the Restore’s website from an expensive website builder to Github Pages, I documented the steps I followed to install Jekyll on my Mac, use the Minimal Mistakes theme and build and modify the website on my machine. See here.\n\n\nConclusion\nI hope this brief introduction gave you a better idea on how to begin and what helpful tools, and difficulties…, lay in front of you. Maybe we do a live hands-on website building workshop in group meeting soon?\nCheers,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2023-11-23-tech-news-5/index.html",
    "href": "posts/tech-news/2023-11-23-tech-news-5/index.html",
    "title": "Tech news 5: Working with data files too large for memory",
    "section": "",
    "text": "Today, I’d like to discuss data we might need that are larger than memory, or fit in memory but everything freezes…\nIt could be a very large data set such as BioTIME, GIS data, model results or simulation results that are accidentally huge but you still want to have a look to what’s inside.\nJust a peek inside First, you could have a look inside a csv by reading only parts of it: loading only some columns and/or only some rows.\nR&gt; library(data.table)\nR&gt; fread(file = “data/big_file.csv”,\n         select = c(“site”,”year”,”temperature”))\n# select has its opposite argument drop\n\n# Reading the column names and types ; empty columns\nR&gt; fread(file = “data/big_file.csv”, nrows = 0)\n\n# Reading the first 100 rows\nR&gt; fread(file = “data/big_file.csv”, nrows = 100)\n\n# Reading rows 10001 to 10101\nR&gt; fread(file = “data/big_file.csv”, skip = 10000,\n         nrows = 100)\nUsing factors instead of character can save quite a lot of memory space too:\nR&gt; library(data.table)\nR&gt; fread(file = \"data/communities_raw.csv\",\n         stringsAsFactors = FALSE) |&gt;\n    object.size() / 10^6\n414.4 Mbytes\nR&gt; fread(file = \"data/communities_raw.csv\",\n         stringsAsFactors = TRUE) |&gt;\n    object.size() / 10^6\n271.1 Mbytes\nThe function is called fread because it reads fast, using several cores if available, it’s very smart at guessing types and it shows a progress bar on large files.\n\nSmaller than memory but dplyr is slow?\nMaybe your dplyr data wrangling step before getting analyses done is taking a few minutes or a few hours and you wouldn’t mind trying to speed things up without having to rewrite everything…\ntidyverse developers too and they created dtplyr to help everyone with that. Add library(dtplyr) at the beginning of your script, lazy_dt(your_data) and bam all your dplyr verbs are going to be translated into data.table calls in the background, you won’t have to change anything else in your script… data.table may be faster thanks to two advantages: 1) fast distributed functions such as mean() and many others and 2) the ability to make operation by reference ie without your column having to be copied in a different place in the memory and a new spot being booked to write the result of your operation because it’s all done in the same place in memory.\n\n\nLarger than memory using Arrow\nArrow is a cross-language multi-platform suite of tools to work on in-memory and larger-than-memory files written in C++.\nYou can use it to access large files and make your usual data wrangling operations on it, even using dplyr verbs.\nFirst read your data with one of the arrow functions:\n\nread_delim_arrow(): read a delimited text file\nread_csv_arrow(): read a comma-separated values (CSV) file\nread_tsv_arrow(): read a tab-separated values (TSV) file\nread_parquet(): read a file in Parquet format\nread_feather(): read a file in Arrow/Feather format\n\nArrow can read the whole data OR read some informations about the data set but without loading the data in memory, only column names, types, sizes, things like that.\nNow you want to make operations on these data and R couldn’t because they don’t fit in memory but arrow is going to read your operations, translate them and execute them all at once when needed:\nlibrary(dplyr)\ndset &lt;- dset %&gt;%\n  group_by(subset) %&gt;%\n  summarize(mean_x = mean(x), min_y = min(y)) %&gt;%\n  filter(mean_x &gt; 0) %&gt;%\n  arrange(subset)\n# No operations were executed yet\ndset %&gt;% collect() # operations are executed and results given\nOnly once dplyr::collect() is called the operations are ran outside of R by arrow. Meaning that the workflow can be much longer and have (much) more intermediate steps but data are loaded in memory for R only when it needs them like for plotting or running a statistical analysis.\n\n\nSpatial Data\nHere we will be looking at sf, stars (for rasters) and dbplyr (as in databases…) and it is a little more advanced and specialised so I won’t go into much details but a few things I liked: Cropping a spatial object even before loading it into R using the wkt_filter argument of sf::st_read()\nlibrary(sf)\nfile &lt;- \"data/nc.gpkg\"\nc(xmin = -82,ymin = 36, xmax = -80, ymax = 37) |&gt;\n    st_bbox() |&gt; st_as_sfc() |&gt; st_as_text() -&gt; bb\nst_read(file, wkt_filter = bb) |&gt; nrow()\n17 # out of 100\nEven easier if you can write SQL queries directly:\nq &lt;- \"select BIR74,SID74,geom from 'nc.gpkg' where BIR74 &gt; 1500\"\nread_sf(file, query = q) |&gt; nrow()\n61 # out of 100\nUsing stars, you can read a raster file without loading it into memory, this is quite similar to arrow in the previous section, and a 100+Mbytes file results in a 12Mbytes object in memory in R.\n\n\nOther interesting tools\nIt seems that packages dedicated to running statistical models (lm, glm, etc) directly on data sets too big for memory were a thing a few years ago but I can’t find recent packages targeting this problem… biglm hasn’t been updated since 2020.\n\n\nGreat resources\n\ndtplyr\n\nhttps://dtplyr.tidyverse.org/\n\n\n\nArrow and dplyr\n\nhttps://arrow.apache.org/docs/r/articles/arrow.html#analyzing-arrow-data-with-dplyr\n\nhttps://arrow.apache.org/cookbook/r/index.html\n\nhttps://arrow-user2022.netlify.app/hello-arrow\n\nhttps://hbs-rcs.github.io/large_data_in_R/#solution-example # examples\n\nhttps://jthomasmock.github.io/arrow-dplyr/#/ # presentation\nhttps://posit-conf-2023.github.io/arrow/materials/5_arrow_single_file.html#/single-file-api # presentation\n\nhttps://www.r-bloggers.com/2021/09/understanding-the-parquet-file-format/ #parquet data format\n\n\n\nSpatial data\n\nhttps://r-spatial.org/book/09-Large.html\n\nHappy to talk about it, share experiences, help you implement something (don’t you also think dtplyr sounds like a great and easy tool?!) and hear your comments!\nBest wishes,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2024-01-26-tech-news-7/index.html",
    "href": "posts/tech-news/2024-01-26-tech-news-7/index.html",
    "title": "Tech news 7: tidymodels",
    "section": "",
    "text": "This week’s email is about tidymodels.\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\nTidymodels offers a consistent and flexible framework for your data science and data programming needs. This tool suite is designed to streamline and simplify the process of building and tuning models, making it easier for researchers to extract meaningful insights from their data.\nTo illustrate the use of key packages in tidymodels, here’s a short example workflow:\nWorkflow (The workflow package enables you to assemble the individual components of your modeling process into a cohesive workflow. You can define the order of operations and specify the preprocessing steps, model fitting, and model evaluation) {\nRecipes (The recipes package helps with data preprocessing and feature engineering. You can create a recipe to define the steps for encoding categorical variables, scaling numeric variables, and creating new features) %&gt;%\nDials (The dials package provides a way to define tuning parameters for models. You can create a set of possible values for each parameter using the grid_*() functions) %&gt;%\nParsnip (The parsnip package allows you to specify the type of model you want to build. For example, you can create a linear regression model using linear_reg() function) %&gt;%\nTune (The tune package provides functions for hyperparameter tuning. You can use the ’tune_*()’ functions to automatically search for the best combination of hyperparameters for your model using a specified tuning grid) }\nBy utilising these packages in sequence, you can build a comprehensive modeling pipeline that includes specifying the model type, preprocessing data, tuning hyperparameters, and evaluating model performance.\nHere is a code example from the tidymodels “Get started” page:\nurchins &lt;- # Loading the data\n  readr::read_csv(\"https://tidymodels.org/start/models/urchins.csv\") %&gt;% \n  stats::setNames(c(\"food_regime\", \"initial_volume\", \"width\")) %&gt;% \n  dplyr::mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n\n# Then preparing the model: a linear regression\nparsnip::linear_reg() %&gt;% # This is the model type\n  parsnip::set_engine(\"keras\")  # This is the engine / fitting method\nlm_mod &lt;- parsnip::linear_reg() # The default engine: “lm” is used\n\n# And now we fit the model:\nlm_fit &lt;- \n  lm_mod %&gt;% \n  fit(width ~ initial_volume * food_regime, data = urchins)\nlm_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = width ~ initial_volume * food_regime, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                    (Intercept)                  initial_volume  \n#&gt;                      0.0331216                       0.0015546  \n#&gt;                 food_regimeLow                 food_regimeHigh  \n#&gt;                      0.0197824                       0.0214111  \n#&gt;  initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n#&gt;                     -0.0012594                       0.0005254\n\n# broom::tidy() offers a clean output:\nbroom::tidy(lm_fit)\n#&gt; # A tibble: 6 × 5\n#&gt;   term                                            estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;                                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)                                   0.0331     0.00962      3.44  0.00100 \n#&gt; 2 initial_volume                                0.00155   0.000398      3.91  0.000222\n#&gt; 3 food_regimeLow                                0.0198      0.0130      1.52  0.133   \n#&gt; 4 food_regimeHigh                               0.0214      0.0145      1.47  0.145   \n#&gt; 5 initial_volume:food_regimeLow                -0.00126   0.000510     -2.47  0.0162  \n#&gt; 6 initial_volume:food_regimeHigh                0.000525  0.000702      0.748 0.457\nNow what if we wanted something different like a Bayesian model? No need to change everything, tidymodels functions are generic even though the underlying packages use very different syntax:\n# set the prior distribution\nprior_dist &lt;- rstanarm::student_t(df = 1)\n\n# make the parsnip model\nbayes_mod &lt;-   \n  parsnip::linear_reg() %&gt;% \n  parsnip::set_engine(\"stan\", \n             prior_intercept = prior_dist, \n             prior = prior_dist) \n\n# train the model\nbayes_fit &lt;- \n  bayes_mod %&gt;% \n  fit(width ~ initial_volume * food_regime, data = urchins)\n\nEasily create training and testing data sets with rsamples:\nurchins_split &lt;- rsamples::initial_split(urchins %&gt;% select(-width), \n                            strata = class)\nurchins_train &lt;- rsamples::training(urchins_split)\nurchins_test  &lt;- rsamples::testing(urchins_split)\nCreate a Workflow for even more reproducibility and reliability:\nurchins_wflow &lt;- \n  workflow::workflow() %&gt;% \n  workflow::add_model(lm_mod) %&gt;% \n  workflow::add_recipe(urchins_rec)\nFit the model on the training data:\nurchins_fit &lt;- \n  urchins_wflow %&gt;% \n  fit(data = train_data)\nAnd easily fit and predict on the test data!\npredict(urchins_fit, test_data)\nI hope this triggered your curiosity and you see potential for an increase in efficiency and reliability in your analytical work!\nI relied heavily on the tidymodels “Get started” page to prepare this email and I encourage anyone interested to consult it too or even follow the introductory or advanced workshops the authors published online!\n\nResources\nThe tidyverse blog is a great resource for staying up to date with the latest tidymodels news and developments:\n\nTidymodels website: https://www.tidymodels.org/\nTidymodels get started: https://www.tidymodels.org/start/\nTidymodels workshops: https://workshops.tidymodels.org/\n\nHappy to discuss it and collaborate!\nBest wishes,\nAlban"
  },
  {
    "objectID": "posts/tech-news/2024-03-14-tech-news-9/index.html",
    "href": "posts/tech-news/2024-03-14-tech-news-9/index.html",
    "title": "Tech news 9: Testing data",
    "section": "",
    "text": "Why testing data?\nLooking inside the data you receive and produce is an absolute necessity but if you could have a second pair of eyes able to scan millions of rows in fractions of seconds and as often as needed, why not? Plus buildings your tests, ie data checks needs you to think forward to define your expectations for data and this helps controlling the workflow and ensuring data quality at each step.\n\n\nHow?\nIn software development, unit tests check that a function always behaves as expected. Unit tests can be extended to check data too, ie:\n\nAre column names equal to “year” and “site” AND are all “year” values positive integers &gt; 1998 AND is the “site” column of type factor?\nAre the X trait values for species 1 in the range 10:20 AND in the range 12:30 for species 2?\n\nBy scripting as many tests as needed, we effortlessly make sure that entry data quality is optimal and that it stays correct along data processing. It is more reproducible and time saving.\n\n\nAssertr for inline/in-workflow testing\nAn example of input data quality control from the assertr documentation:\n\nLet’s say, for example, that the R’s built-in car dataset, mtcars, was not built-in but rather procured from an external source that was known for making errors in data entry or coding. Pretend we wanted to find the average miles per gallon for each number of engine cylinders. We might want to first, confirm - that it has the columns “mpg”, “vs”, and “am” - that the dataset contains more than 10 observations - that the column for ‘miles per gallon’ (mpg) is a positive number - that the column for ‘miles per gallon’ (mpg) does not contain a datum that is outside 4 standard deviations from its mean, and - that the “am” and “vs” columns (automatic/manual and v/straight engine, respectively) contain 0s and 1s only - each row contains at most 2 NAs - each row is unique jointly between the “mpg”, “am”, and “wt” columns - each row’s mahalanobis distance is within 10 median absolute deviations of all the distances (for outlier detection) This could be written (in order) using assertr like this:\n\nlibrary(dplyr)\nlibrary(assertr)\n\nmtcars %&gt;%\n  verify(has_all_names(\"mpg\", \"vs\", \"am\", \"wt\")) %&gt;%\n  verify(nrow(.) &gt; 10) %&gt;%\n  verify(mpg &gt; 0) %&gt;%\n  insist(within_n_sds(4), mpg) %&gt;%\n  assert(in_set(0,1), am, vs) %&gt;%\n  assert_rows(num_row_NAs, within_bounds(0,2), everything()) %&gt;%\n  assert_rows(col_concat, is_uniq, mpg, am, wt) %&gt;%\n  insist_rows(maha_dist, within_n_mads(10), everything()) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n\nIf any of these assertions were violated, an error would have been raised and the pipeline would have been terminated before calculation happened.\n\nIn this workflow, assertr was used for inline testing, it is immediately apparent, transparent and clear. But maybe you would rather have a suite of tests in a separate script that you would source() or that you would call with the dedicated and enriched with clear and helpful error messages testthat::test_file().\n\n\nTest suites called by testthat\ntestthat was developed for software unit testing and it is the reference in R package building but extensions make it a highly efficient data testing tool. The usual testthat test suite is structured around expectations:\n\nthe rpois function is expected to error if argument x has NA\nthe rpois function is expected to return a vector of positive integers without NAs\n\nlibrary(testthat)\ntest_that(“rpois behaves as expected”, {\n   expect_error(rpois(n = NA, 10))\n\n   expect_false(anyNA(rpois(n = 5,10)))\n   expect_vector(rpois(n = 5,10), size = 5)\n   expect_gte(rpois(n = 5, 10), 0))\n   expect_type(rpois(n = 5, 10), “integer”)\n})\nNow, if we could have more data oriented tests, easily applied to data frames, it would feel more natural than multiplying testthat expectations. For example, the last four testthat expectations could be rewritten with only one checkmate expectation:\ncheckmate::expect_integer(rpois(n = 5, 10), lower = 0, len = 5, any_missing = FALSE)\nAnother useful checkmate function to check column names: Expecting all column names in any order\nexpect_names(\n  permutation.of = c(“region”,”site”,”plot”,”year”,”month”),\n  what = \"colnames\"\n)\nOr expecting all names AND in order\nexpect_names(\n  identical.to = c(“region”,”site”,”plot”,”year”,”month”),\n  what = \"colnames\"\n)\nOr allowing only a subset of a list\nexpect_names(\n  subset.of = c(“region”,”site”,”plot”,”year”,”month”),\n  what = \"colnames\"\n)\nCheckmate was developed with focus on helpful error messages and efficiency:\n\nVirtually every standard type of user error when passing arguments into function can be caught with a simple, readable line which produces an informative error message in case. A substantial part of the package was written in C to minimize any worries about execution time overhead. Furthermore, the package provides over 30 expectations to extend the popular testthat package for unit tests.\n\nNow, let’s use testdat, another extension to testthat, to build a testing suite for data. First, the testdat authors give an example workflow in which the user creates data objects and visually and interactively checks them:\nlibrary(dplyr)\n\nx &lt;- tribble(\n  ~id, ~pcode, ~state, ~nsw_only,\n  1,   2000,   \"NSW\",  1,\n  2,   3123,   \"VIC\",  NA,\n  3,   2123,   \"NSW\",  3,\n  4,   12345,  \"VIC\",  3\n)\n\n# check id is unique\nx %&gt;% filter(duplicated(id))\n\n# check values\nx %&gt;% filter(!pcode %in% 2000:3999)\nx %&gt;% count(state)\nx %&gt;% count(nsw_only)\n\n# check base for nsw_only variable\nx %&gt;% filter(state != \"NSW\") %&gt;% count(nsw_only)\n\nx &lt;- x %&gt;% mutate(market = case_when(pcode %in% 2000:2999 ~ 1,\n                                     pcode %in% 3000:3999 ~ 2))\n\nx %&gt;% count(market)\nAnd then using testdat:\nlibrary(testdat)\nlibrary(dplyr)\n\nx &lt;- tribble(\n  ~id, ~pcode, ~state, ~nsw_only,\n  1,   2000,   \"NSW\",  1,\n  2,   3123,   \"VIC\",  NA,\n  3,   2123,   \"NSW\",  3,\n  4,   12345,  \"VIC\",  3\n)\n\nwith_testdata(x, {\n  test_that(\"id is unique\", {\n    expect_unique(id)\n  })\n  \n  test_that(\"variable values are correct\", {\n    expect_values(pcode, 2000:2999, 3000:3999)\n    expect_values(state, c(\"NSW\", \"VIC\"))\n    expect_values(nsw_only, 1:3) # by default expect_values allows NAs\n  })\n  \n  test_that(\"filters applied correctly\", {\n    expect_base(nsw_only, state == \"NSW\")\n  })\n})\nWhere attention is needed only if a test fails… these tests can be stored in a separate script and called with the data they are applied to. Finally, in heavy data workflows where input data, received from data providers or automatic loggers, follows a limited number of formats, full data check suites can be automatically ran on reception with rich and parametrisable reporting and this is what pointblank was developed for.\nChecking the data your data providers send/your automatic inputs\n\nYou can also use pointblank in your script as shown here:\nlibrary(pointblank)\ndplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  col_vals_between(\n    a, 1, 9,\n    na_pass = TRUE,\n    actions = warn_on_fail()\n  ) %&gt;%\n  col_vals_lt(\n    c, 12,\n    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b),\n    actions = warn_on_fail()\n  ) %&gt;%\n  col_is_numeric(\n    c(a, b),\n    actions = warn_on_fail()\n  )\nBut a better use for it is creating so-called agents that can automatically and reproducibly test data and provide rich visual reports just like this:\nagent &lt;- \n  dplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  create_agent(\n    label = \"A very *simple* example.\",\n    actions = al\n  ) %&gt;%\n  col_vals_between(\n    vars(a), 1, 9,\n    na_pass = TRUE\n  ) %&gt;%\n  col_vals_lt(\n    vars(c), 12,\n    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b)\n  ) %&gt;%\n  col_is_numeric(vars(a, b)) %&gt;%\n  interrogate()\nprint(agent)\nAnd the pointblank package currently supports PostgreSQL. MySQL, MariaDB, Microsoft SQL Server, Google BigQuery, DuckDB, SQLite, and Spark DataFrames!\n\n\n\nResources\nhttps://ropensci.r-universe.dev/assertr#\nhttps://testthat.r-lib.org/\nhttps://socialresearchcentre.r-universe.dev/testdat#\nhttps://socialresearchcentre.github.io/testdat/articles/testdat.html\nhttps://rstudio.r-universe.dev/pointblank#"
  },
  {
    "objectID": "posts/tech-news/2024-05-24-tech-news-11/index.html",
    "href": "posts/tech-news/2024-05-24-tech-news-11/index.html",
    "title": "Tech news 11: Facilitating… admin tasks",
    "section": "",
    "text": "Filling in PDF forms and sending personalised emails\n\n\nWhat for?\nMaybe you want to pre-fill travel requests for all members of your group? Maybe you want to send a personalised email to all of your 200 co-authors, data providers, citizen science participants or parliament representatives? Maybe you want to send exam subjects to students but the data have to be different and randomly affected?\n\n\nA case study\nWhile our manuscript was under review at GEB we decided to offer co-authorship to data providers that had not been contacted before because their data were open access. Why should they not be proposed to be coauthors while we propose it to people whom we had to contact to get access to their data, right?\nGEB then asked us to send the the new list of co-authors to all co-authors, original and new. They would have to physically sign it, digital signatures are not acceptable. And this is a Wiley process, not just GEB, nut fun Wiley!\nSo anyway, two tasks that would be painful to do by hand: 1) reporting name, email address and institution of all 121 co-authors in a table with only nine rows and also reporting the names of the 49 new authors in another table with only five rows ; and 2) sending personalised messages to all of them.\n\n\nThe data\nBoth processes could only be automated with good data and we had a table with separated first and family names, institutions and verified email addresses. It really helped that first names and family names were already in two columns and that multiple institutions were also in distinct columns.\n\n\nPDF form filling in R\nAt least the Wiley form had fields and after looking into the packages I already had on hand: pdftools and qpdf, I turned to DuckDuckGo that pointed me in direction of the staplr package. The installation was not painless, mostly because staplr depends on java and maybe there was a conflict with pdftools but once set, staplr did everything!\nIt has basic pdf tools such as select_pages(), remove_pages(), rotate_pages() and combine_pdf() that we used to extract the pages with the tables before multiplying the first table 14 times and the second one 10 times, filling them and finally combining all pages together. For filling in the pdf, staplr works in three steps:\n# First reading all fields in the document, there are 116 fields\n&gt; fields &lt;- staplr::get_fields(input_filepath = \"Authorship-change-form--1---5---2--1.pdf\")\n&gt; fields[[50]] # this is Table 2, column `Email address`, row 6\n$type\n[1] \"Text\"\n\n$name\n[1] \"Email addressRow6\"\n\n$value\n[1] \"\"\n# Then, filling in the fields in the value slot like this:\nfields[[50]]$value &lt;- \"alban.sagouis@idiv.de\"\n# Easy! But looping over 5 columns, 121 authors, changing page every 9 authors took some more tweaking…\n# And finally writing the fields back inside the PDF document!\nstaplr::set_fields(\n         input_filepath = \"Authorship-change-form--1---5---2--1.pdf\",\n         fields = fields,\n         output_filepath = \"filled_authorship_form.pdf\")\n\n\nBatch sending personalised emails\nNow that we have the form ready, we can write to all co-authors, asking them to kindly print, sign, scan and send back the form. Our message would begin with “Dear Dr {Family name author}, thank you for contributing data from your data set entitled {Dataset title}, et caetera”. I quickly looked into how to do it in R and the prospect of setting up an email server seemed too complicated, especially when Microsoft Word and Outlook can do it.\nJust write your message in a Word document, click on the Mailings tab, Start Mail Merge, choose Letters or Email messages and then click on Select Recipients and select the table where you stored the names, the titles, the personalised messages, random data for an exam, everything you need personalised and of course, the email addresses. Every time you reach a part of the email where you want personalised text, just Insert Merge Field and select the column from your table where to look for data. Once you’re done composing the email, you can Preview Results and go through all 121 recipients to check that the good name or title is sent to the good email address or check that special characters were properly reproduced.\nIt’s that simple! Finish & Merge, Word and Outlook are going to talk to each other and send each email independently.\nOh, you want to send an attachment too? Or even send a different attachment to each person? Well, Word and Outlook won’t give you the option 🤷️. We tried a first add-in for Word whose free plan did not fit our needs (max 50 recipients) and then found the Merge Tools add-in that allows batch sending emails 20 times for free, thanks Merge Tools.\n\n\nEncoding\nWhen reading data, Word preferred csv to xlsx but encoding handling was better from an Excel file… For one of the steps, Merge Tools seemed to mishandle UTF-8 encoding and I had to remove all special characters ie. “èéêëě” all became “e”. I used this R command:\nstringi::stri_trans_general(\n   str = your_string_with_special_characters, \n   id = “Latin-ASCII\")\nIf you have tidyverse installed, you already have stringi.\n\n\nResources\nThe staplr package: https://cran.r-project.org/web/packages/staplr/\nThe microsoft Support page for Mail Merge: https://support.microsoft.com/en-us/office/use-mail-merge-to-send-bulk-email-messages-0f123521-20ce-4aa8-8b62-ac211dedefa4\nA quick video to see it happening: https://www.youtube.com/watch?v=NikC2cJ_tHQ\nThe Merge Tools add-in that we ended up using: https://mergetoolsaddin.com/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Email\n  \n  \n    \n     Github\n  \n  \n    \n     Zenodo\n  \n  \n    \n     ORCID 0000-0002-3827-1063\n  \n  \n    \n     ResearchGate\n  \n  \n    \n     iDiv\n  \n  \n    \n     Linkedin\n  \n\n  \n  \nMy name is Alban Sagouis and this is my personal professional website.\nThere is a dynamic CV that aggregates my blog posts described as professional experiences and also blog posts about photography and other topics.\n\nCurrent job\nI currently work at iDiv, the German Biodiversity Synthesis Center in Jonathan Chase’s group: Biodiversity Synthesis.\nAs a Scientific programmer:\n\nI extract, aggregate, restructure, check and standardise data from the literature.\nI assist colleagues with R problems.\nI implement and encourage scientific software reproducibility practices, notably containerisation and automatic testing.\nI frequently talk about how great is the renv package.\nI use all of the above to build and grow research compendia extracting, restructuring, standardising and checking data in a documented and reproducible environment, publishing and archiving code and data.\nI write monthly newsletters to my colleagues which you can find here.\nI occasionally build a package of functions used for a specific project such as the sRealmTools package or BioTIMEr.\nI co-maintain mobsim, a community simulation R package that we use for work, for teaching and published on CRAN.\n\n\n\n\n\n\nOther activities\nWith rOpenSci, I maintain or co-maintain parzer and rgnparser.\n\n\nBuilding this website\nI initially used Jekyll and the template Minimal Mistake by Michael Rose and developed and built locally in Visual Studio Code. I wrote about this process in a blog post.\nThis version of the website was built using Quarto which seems easier to me and better integrated in my workflow."
  }
]